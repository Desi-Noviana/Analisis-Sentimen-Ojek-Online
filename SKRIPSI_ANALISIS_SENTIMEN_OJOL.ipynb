{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOhuQGRQMhj+0rF73suKibD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Desi-Noviana/Analisis-Sentimen-Ojek-Online/blob/main/SKRIPSI_ANALISIS_SENTIMEN_OJOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swifter"
      ],
      "metadata": {
        "id": "Olhc8fyFeyUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install plotly-express"
      ],
      "metadata": {
        "id": "X-QMQq57e2nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "VdOEhQf1e5kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "v-IWUKkDe87U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "PMXvWxbWfR44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Sastrawi"
      ],
      "metadata": {
        "id": "LNjzM5HWfbg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import keras\n",
        "import swifter\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam,SGD\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "EVDMvwXqbPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Preprocessing_ojol.csv', error_bad_lines = False)\n",
        "pd.set_option('max_colwidth', None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "p-xCH0_pfmGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualiasi Data"
      ],
      "metadata": {
        "id": "ZPk0ZrjU-U5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(df, x=\"sentiment\", title=\"Jumlah Sentiment\")\n",
        "fig.update_layout(bargap=0.2)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LDtSAqrG2tDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(df.Ulasan)\n",
        "\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'red')\n",
        "plt.title(\"Most Frequently Occuring Words - Top 30\")"
      ],
      "metadata": {
        "id": "KEShJxi72xPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wr = pd.read_csv('Preprocessing_ojol.csv', error_bad_lines = False)"
      ],
      "metadata": {
        "id": "O8yUt3U04lyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wr['sentiment'] = wr['sentiment'].replace('positive', 2)\n",
        "wr['sentiment'] = wr['sentiment'].replace('negative', 0)\n",
        "wr['sentiment'] = wr['sentiment'].replace('neutral', 1)"
      ],
      "metadata": {
        "id": "z8Q7o3nh4u1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.title(\"WordCloud Kata Yang Muncul\", fontsize = 15)"
      ],
      "metadata": {
        "id": "9a7Swz784zAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_words =' '.join([text for text in wr['Ulasan'][wr['sentiment'] == 1]])\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'white',width=1920, height=1080, random_state=5, max_font_size = 500).generate(neutral_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Neutral Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KD5H8V0F4_RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words =' '.join([text for text in wr['Ulasan'][wr['sentiment'] == 2]])\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'white',width=800, height=500, random_state=5, max_font_size = 110).generate(positive_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Positive Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_6wrBOM6cF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words =' '.join([text for text in wr['Ulasan'][wr['sentiment'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'white',width=800, height=500, random_state=5, max_font_size = 110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Negative Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9A7pi5cY6m1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding"
      ],
      "metadata": {
        "id": "RnQu65e4-nDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Preprocessing_ojol.csv', error_bad_lines = False)"
      ],
      "metadata": {
        "id": "lze7rPMl-PbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ulasan = data['Ulasan']"
      ],
      "metadata": {
        "id": "51aNiI4B-3KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(data['sentiment'])\n",
        "y = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'negative':\n",
        "        y.append(0)\n",
        "    if labels[i] == 'neutral':\n",
        "        y.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        y.append(2)\n",
        "y = np.array(y)\n",
        "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"int64\")\n",
        "del y"
      ],
      "metadata": {
        "id": "ZfOQb0s6_BiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "id": "_FsGU_RO_Ewh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proses Data"
      ],
      "metadata": {
        "id": "_kz5ifkW_LTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "#Splitting pd.Series to list\n",
        "data_to_list = ulasan.values.tolist()\n",
        "for i in range(len(data_to_list)):\n",
        "    temp.append((data_to_list[i]))\n",
        "list(temp[:10])"
      ],
      "metadata": {
        "id": "mwYa2NFS_Hbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "        \n",
        "data_words = list(sent_to_words(temp))\n",
        "\n",
        "print(data_words[:10])"
      ],
      "metadata": {
        "id": "Z5-DnSod_SKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_words)"
      ],
      "metadata": {
        "id": "mFjsUH2P_eVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(text):\n",
        "    return TreebankWordDetokenizer().detokenize(text)"
      ],
      "metadata": {
        "id": "fOHLUJUZ_kb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(len(data_words)):\n",
        "    data.append(detokenize(data_words[i]))\n",
        "print(data[:10])"
      ],
      "metadata": {
        "id": "JtIVGWJK_pl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)"
      ],
      "metadata": {
        "id": "1hvrLwg8_sIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mengubah Data Kedalam Bentuk Angka"
      ],
      "metadata": {
        "id": "4y0XZtFX_0GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "ulasan = pad_sequences(sequences, maxlen=max_len)\n",
        "print(ulasan)"
      ],
      "metadata": {
        "id": "W-T4PDOm_zFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(1000, 64)"
      ],
      "metadata": {
        "id": "ltnORmPcAlHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer)"
      ],
      "metadata": {
        "id": "_twXqxsaAoPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(ulasan,labels,test_size=0.2,random_state=0)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ],
      "metadata": {
        "id": "T3CRGD-pAp1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "model1.add(layers.Embedding(max_words, 10))\n",
        "model1.add(layers.LSTM(20,dropout = 0.5,recurrent_dropout=0.5))\n",
        "model1.add(layers.Dense(3, activation='softmax'))\n",
        "model1.summary()\n",
        "\n",
        "\n",
        "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto',period=1,save_weights_only=False)\n",
        "history = model1.fit(X_train, y_train, epochs=30,batch_size=28,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
      ],
      "metadata": {
        "id": "rEdbqfirAzzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
        "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.57,recurrent_dropout=0.57)))\n",
        "model2.add(layers.Dense(3,activation='softmax'))\n",
        "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model2.fit(X_train, y_train, epochs=40,batch_size=28,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
      ],
      "metadata": {
        "id": "QhWRYDMK6uOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menyimpan Model"
      ],
      "metadata": {
        "id": "56IYizp15fg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model(\"best_model1.hdf5\")"
      ],
      "metadata": {
        "id": "oAVcsx_N3xbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Model"
      ],
      "metadata": {
        "id": "JGn71q1756mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "id": "IdiiXWfV56Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        "epochs = range(len(acc))\n",
        " \n",
        "plt.plot(epochs, acc, 'blue', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'red', label='Validation acc')\n",
        "plt.title('Percobaan 1', fontsize=15)\n",
        "plt.legend()\n",
        " \n",
        "plt.figure()\n",
        " \n",
        "plt.plot(epochs, loss, 'blue', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'red', label='Validation loss')\n",
        "plt.legend()\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZeeQKMnl6G8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}